---
title: "Automating Support Messages with LLMs for AUD Intervention"
author: "Christopher Janssen"
date: "April 28, 2025"
format:
  revealjs:
    transition: slide
    footer: "Mini-Prac"
    slide-number: true
    code-fold: false
    theme: decoration
---

## Introduction {.section-slide}

## Context

::: info-box
-   Collaboration with the UW Addiction Research Lab
-   Focus on automating personalized engagement messages
-   Using OpenAI API for message generation
:::

::: notes
This presentation focuses on my contribution to the UW-Madison Addiction Research Center's research on smart digital therapeutics for alcohol use disorder. Specifically, I've developed an R script that connects to the OpenAI's API to automate the generation of personalized support messages to aid in the recovery of those with moderate to server alcohol use disorder.

These messages, and the R script for generating them, serve as important components of this study. The messages function as delivery vehicles for the personalized risk information that forms the experimental foundation of the research, while simultaneously, the automated generation system demonstrates a scalable, cost-effective pathway for implementing this intervention beyond research settings.
:::

## *A-CHESS* & Research Context {.section-slide}

## A-CHESS: A Digital Therapeutic for Alcohol Use Disorder

::::::: {layout="[0.5, 0.5]"}
:::: {#first-column}
**A-CHESS**:

::: incremental
-   Addiction
-   Comprehensive
-   Health
-   Enhancement
-   Support
-   System
:::
::::

:::: {#second-column}
Addresses barriers to treatment:

::: incremental
-   Availability
-   Accessibility
-   Affordability
-   Acceptability
:::
::::
:::::::

::: notes
A-CHESS stands for Comprehensive Health Enhancement Support System for Addiction. Developed at the Center for Health Enhancement Systems Studies, A-CHESS is an evidence-based digital therapeutic for alcohol use disorder. Clinical trials have shown it can reduce risky drinking days by 57% and double the odds of abstinence compared to treatment as usual. What makes A-CHESS particularly valuable is how it addresses the major barriers that prevent many people with AUD from receiving treatment. It provides 24/7 access via smartphones, reaches underserved populations, scales cost-effectively, and reduces the stigma associated with seeking help for AUD.
:::

## Smart A-CHESS: The Next Gen

Evolution of digital therapeutics for AUD

::: highlight-box
-   Innovations:
    -   Embedded machine learning prediction models

    -   Personal sensing capabilities (EMA, geolocation)

    -   Personalized risk assessment

    -   "Just-in-time" adaptive interventions
:::

::: notes
The overarching goal is to optimize how Smart A-CHESS provides feedback to users to maximize both engagement with the app and clinical outcomes.

The current study tests three different components of engagement messages:

-   information about lapse probability

-   important risk features identified by the model

-   and specific module recommendations tailored to address those risk features

The study will include upwards of 400 participants with moderate to severe alcohol use disorder and track both their engagement with the app and their drinking outcomes over six months. This large-scale optimization study will determine which message components should be included in future versions of Smart A-CHESS and similar digital therapeutics.
:::

## Code {.section-slide}

![](../images/airplane1.png)

## The `flight_support.R` Script {.smaller auto-animate="true"}

``` {.r .scrollable}
{
  if(!exists("runway_cleared", envir = .GlobalEnv)) {
    clear_runway <- function() {
      required_packages <- c(
        "httr2",     
        "tidyverse"  
      )
      
      for(pkg in required_packages) {
        if(!requireNamespace(pkg, quietly = TRUE)) {
          message(paste0("Installing package: ", pkg))
          install.packages(pkg)
        }
        library(pkg, character.only = TRUE)
      }
      
      message("All required packages loaded successfully. Ready for takeoff!")
    }
    
    clear_runway()
    assign("runway_cleared", TRUE, envir = .GlobalEnv)
  }
}

airport_security <- function() {
  if(!exists("flight_credentials", envir = .GlobalEnv)) {
    # check if api file exists
    if(file.exists("azure.api")) {
      
      api <- readr::read_csv("azure.api", show_col_types = FALSE)      
      api_key <- api$key[[1]] 
      endpoint <- api$endpoint[[1]] 
    } else {  # input api details
      cat("Please provide your flight credentials:\n")
      api_key <- readline("Enter your API key: ")
      endpoint <- readline("Enter your API endpoint: ")
    }
    
    flight_credentials <- list(api_key = api_key,
                               endpoint = endpoint)
    assign("flight_credentials", flight_credentials, envir = .GlobalEnv)
  }
  
  return(get("flight_credentials", envir = .GlobalEnv))
}


set_course <- function(prompt, system_content = NULL, temperature = 0) {
  credentials <- airport_security()
  
  # Ensure temperature is within valid range (0-2)
  temperature <- max(0, min(2, temperature))
  
  messages <- list()
  
  # add system message if provided
  if (!is.null(system_content)) {
    messages[[length(messages) + 1]] <- list(role = "system", content = system_content)
  }
  
  # add user message
  messages[[length(messages) + 1]] <- list(role = "user", content = prompt)
  
  request <- httr2::request(credentials$endpoint) |>
    httr2::req_headers(
      Authorization = paste("Bearer", credentials$api_key),
      "Content-Type" = "application/json"
    ) |>
    httr2::req_body_json(list(
      model = "optimize-v2",
      messages = messages,
      max_tokens = 600,
      temperature = temperature
    ))
  
  response <- httr2::req_perform(request)
  httr2::resp_body_json(response)
}

pilot_message <- function(base_prompt, 
                          system_prompt = NULL,
                          additional_information = NULL,
                          connections = 4,
                          temperature = 0,
                          itinerary = FALSE) {
  
  enhanced_prompt <- base_prompt
  
  if(!is.null(additional_information)) {
    enhanced_prompt <- paste0(enhanced_prompt, 
                              "\nAdditional information: ", additional_information)
  }
  
  all_messages <- list()
  
  for(i in 1:connections) {
    response <- set_course(enhanced_prompt, system_content = system_prompt, temperature = temperature)
    
    if (!is.null(response) && !is.null(response$choices) && length(response$choices) > 0) {
      raw_result <- response$choices[[1]]$message$content
      all_messages[[i]] <- trimws(raw_result)
    } else {
      all_messages[[i]] <- paste("API response error for connection", i)
    }
  }
  
  all_messages <- unlist(all_messages)
  
  if (itinerary) {
    return(list(
      system = system_prompt,
      prompt = enhanced_prompt,
      response = all_messages
    ))
  } else {
    return(all_messages)
  }
}
```

::: notes
The `flight_support.R` script is the core of my contribution. I've created an intuitive R interface that handles connecting to the Azure API, sending properly formatted requests, and processing responses. Much to the chagrin of my collaborators, I've used an aviation theme for function naming. This theme reflects the idea of "piloting" the message generation process and, in my opinion, helps make the code more memorable and engaging for others who might work with it.

The script consists of four main functions, each handling a specific aspect of the workflow. Obviously, the script is rather large for a presentation, so I'll be breaking it down into these four functions
:::

## `clear_runway()`: Setting Up the Environment {.smaller auto-animate="true" transition="none"}

``` {.r .scrollable code-line-numbers="|1-6|7-14|"}
clear_runway <- function() {
  required_packages <- c(
    "httr2",     
    "tidyverse"  
  )
  
  for(pkg in required_packages) {
    if(!requireNamespace(pkg, quietly = TRUE)) {
      message(paste0("Installing package: ", pkg))
      install.packages(pkg)
    }
    library(pkg, character.only = TRUE)
  }
}
```

::: notes
The 'clear_runway()' function handles the installation and loading of required packages. It's designed to run only once per session by checking for a global variable. The function uses a list of required packages and installs any that aren't already present, then loads all of them. This makes the script more user-friendly, especially for collaborators who might not have all dependencies installed. The primary dependencies are httr2 for API communication and tidyverse for data manipulation.
:::

## `airport_security()`: API Access {.smaller auto-animate="true" transition="none"}

``` {.r .scrollable code-line-numbers="|2|3-6|8-11|12-16|"}
airport_security <- function() {
  if(!exists("flight_credentials", envir = .GlobalEnv)) {
    if(file.exists("azure.api")) {
      api <- readr::read_csv("azure.api", show_col_types = FALSE)      
      api_key <- api$key[[1]] 
      endpoint <- api$endpoint[[1]] 
    } else { 
      cat("Please provide your flight credentials:\n")
      api_key <- readline("Enter your API key: ")
      endpoint <- readline("Enter your API endpoint: ")
    }
    
    flight_credentials <- list(api_key = api_key,
                               endpoint = endpoint)
    assign("flight_credentials", flight_credentials, envir = .GlobalEnv)
  }
  
  return(get("flight_credentials", envir = .GlobalEnv))
}
```

::: notes
The 'airport_security()' function handles the sensitive API credentials. It first checks if credentials are already stored in the global environment from a previous call. If not, it looks for a file named 'azure.api' that would contain the key and endpoint. If that file doesn't exist, it prompts the user to enter the credentials manually. Once obtained, the credentials are stored in the global environment for the duration of the session. This approach balances security with convenience, avoiding hardcoded credentials while not requiring repeated authentication.
:::

## `set_course()`: Crafting and Sending Copilot API Requests {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable}
set_course <- function(prompt, system_content = NULL, temperature = 0) {
  credentials <- airport_security()
  
  temperature <- max(0, min(2, temperature))
  
  messages <- list()
  
  if (!is.null(system_content)) {
    messages[[length(messages) + 1]] <- list(role = "system", content = system_content)
  }
  
  messages[[length(messages) + 1]] <- list(role = "user", content = prompt)
  
  request <- httr2::request(credentials$endpoint) |>
    httr2::req_headers(
      Authorization = paste("Bearer", credentials$api_key),
      "Content-Type" = "application/json"
    ) |>
    httr2::req_body_json(list(
      model = "optimize-v2",
      messages = messages,
      max_tokens = 600,
      temperature = temperature
    ))
  
  response <- httr2::req_perform(request)
  httr2::resp_body_json(response)
}
```
:::

::: notes
The 'set_course()' function is where we build and send the actual API request. It accepts three parameters: the user prompt, an optional system content message to guide the model's behavior, and a temperature parameter to control output "randomness". The function first retrieves the necessary credentials, then builds a list of messages with the appropriate format for the Azure API. It supports both system and user message roles, making it flexible for different prompting strategies. The request is formatted using httr2, with appropriate headers and body content, and then sent to the API. The response is then parsed from JSON and returned.
:::

## `set_course()`: Crafting and Sending Copilot API Requests {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable code-line-numbers="|1|2|6-12|"}
set_course <- function(prompt, system_content = NULL, temperature = 0) {
  credentials <- airport_security()
  
  temperature <- max(0, min(2, temperature))
  
  messages <- list()
  
  if (!is.null(system_content)) {
    messages[[length(messages) + 1]] <- list(role = "system", content = system_content)
  }
  
  messages[[length(messages) + 1]] <- list(role = "user", content = prompt)
```
:::

::: notes
The 'set_course()' function is where we build and send the actual API request. It accepts three parameters: the user prompt, an optional system content message to guide the model's behavior, and a temperature parameter to control output "randomness". The function first retrieves the necessary credentials, then builds a list of messages with the appropriate format for the Azure API. It supports both system and user message roles, making it flexible for different prompting strategies. The request is formatted using httr2, with appropriate headers and body content, and then sent to the API. The response is then parsed from JSON and returned.
:::

## `set_course()`: Crafting and Sending Copilot API Requests {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.9em; max-height: 400px;"}
``` {.r .scrollable code-line-numbers="|1-11|12-15|"}
  request <- httr2::request(credentials$endpoint) |>
    httr2::req_headers(
      Authorization = paste("Bearer", credentials$api_key),
      "Content-Type" = "application/json"
    ) |>
    httr2::req_body_json(list(
      model = "optimize-v2",
      messages = messages,
      max_tokens = 600,
      temperature = temperature
    ))
  
  response <- httr2::req_perform(request)
  httr2::resp_body_json(response)
}
```
:::

::: notes
The 'set_course()' function is where we build and send the actual API request. It accepts three parameters: the user prompt, an optional system content message to guide the model's behavior, and a temperature parameter to control output "randomness". The function first retrieves the necessary credentials, then builds a list of messages with the appropriate format for the Azure API. It supports both system and user message roles, making it flexible for different prompting strategies. The request is formatted using httr2, with appropriate headers and body content, and then sent to the API. The response is then parsed from JSON and returned.
:::

## `set_course()`: Crafting and Sending Copilot API Requests {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable}
set_course <- function(prompt, system_content = NULL, temperature = 0) {
  credentials <- airport_security()
  
  temperature <- max(0, min(2, temperature))
  
  messages <- list()
  
  if (!is.null(system_content)) {
    messages[[length(messages) + 1]] <- list(role = "system", content = system_content)
  }
  
  messages[[length(messages) + 1]] <- list(role = "user", content = prompt)
  
  request <- httr2::request(credentials$endpoint) |>
    httr2::req_headers(
      Authorization = paste("Bearer", credentials$api_key),
      "Content-Type" = "application/json"
    ) |>
    httr2::req_body_json(list(
      model = "optimize-v2",
      messages = messages,
      max_tokens = 600,
      temperature = temperature
    ))
  
  response <- httr2::req_perform(request)
  httr2::resp_body_json(response)
}
```
:::

::: notes
The 'set_course()' function is where we build and send the actual API request. It accepts three parameters: the user prompt, an optional system content message to guide the model's behavior, and a temperature parameter to control output "randomness". The function first retrieves the necessary credentials, then builds a list of messages with the appropriate format for the Azure API. It supports both system and user message roles, making it flexible for different prompting strategies. The request is formatted using httr2, with appropriate headers and body content, and then sent to the API. The response is then parsed from JSON and returned.
:::

## `pilot_message()`: Generating the Messages {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable}
pilot_message <- function(base_prompt, 
                         system_prompt = NULL,
                         additional_information = NULL,
                         connections = 4,
                         temperature = 0,
                         itinerary = FALSE) {
  
  enhanced_prompt <- base_prompt
  
  if(!is.null(additional_information)) {
    enhanced_prompt <- paste0(enhanced_prompt, 
                             "\nAdditional information: ", additional_information)
  }
  
  all_messages <- list()
  
  for(i in 1:connections) {
    response <- set_course(enhanced_prompt, system_content = system_prompt, 
                         temperature = temperature)
    
    if (!is.null(response) && !is.null(response$choices) && 
        length(response$choices) > 0) {
      raw_result <- response$choices[[1]]$message$content
      all_messages[[i]] <- trimws(raw_result)
    } else {
      all_messages[[i]] <- paste("API response error for connection", i)
    }
  }
  
  all_messages <- unlist(all_messages)
  
  if (itinerary) {
    return(list(
      system = system_prompt,
      prompt = enhanced_prompt,
      response = all_messages
    ))
  } else {
    return(all_messages)
  }
}
```
:::

::: notes
The 'pilot_message()' function serves as the main user interface. It's designed to generate multiple message variations in a single call, providing flexibility for researchers to choose the best response.

The function accepts several parameters: the base prompt, an optional system prompt for guided generation, additional information to append to the prompt, the number of message variations to generate, temperature for controlling randomness, and an verbose flag called "itinerary".

For each requested connection, it calls set_course() and collects the results. The function handles error checking and provides either just the generated messages or a complete list with the original prompts depending on the itinerary parameter.
:::

## `pilot_message()`: Generating the Messages {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable code-line-numbers="|1|2|3|4|5|6|8-15"}
pilot_message <- function(base_prompt, 
                         system_prompt = NULL,
                         additional_information = NULL,
                         connections = 4,
                         temperature = 0,
                         itinerary = FALSE) {
  
  enhanced_prompt <- base_prompt
  
  if(!is.null(additional_information)) {
    enhanced_prompt <- paste0(enhanced_prompt, 
                             "\nAdditional information: ", additional_information)
  }
  
  all_messages <- list()
```
:::

::: notes
The 'pilot_message()' function serves as the main user interface. It's designed to generate multiple message variations in a single call, providing flexibility for researchers to choose the best response.

The function accepts several parameters: the base prompt, an optional system prompt for guided generation, additional information to append to the prompt, the number of message variations to generate, temperature for controlling randomness, and an verbose flag called "itinerary".

For each requested connection, it calls set_course() and collects the results. The function handles error checking and provides either just the generated messages or a complete list with the original prompts depending on the itinerary parameter.
:::

## `pilot_message()`: Generating the Messages {.smaller auto-animate="true" transition="none"}

::: {.code-overflow-scroll style="font-size: 0.8em; max-height: 400px;"}
``` {.r .scrollable code-line-numbers="|1-4|5-13|13-25|"}
  for(i in 1:connections) {
    response <- set_course(enhanced_prompt, system_content = system_prompt, 
                         temperature = temperature)
    
    if (!is.null(response) && !is.null(response$choices) && 
        length(response$choices) > 0) {
      raw_result <- response$choices[[1]]$message$content
      all_messages[[i]] <- trimws(raw_result)
    } else {
      all_messages[[i]] <- paste("API response error for connection", i)
    }
  }
  
  all_messages <- unlist(all_messages)
  
  if (itinerary) {
    return(list(
      system = system_prompt,
      prompt = enhanced_prompt,
      response = all_messages
    ))
  } else {
    return(all_messages)
  }
}
```
:::

::: notes
The 'pilot_message()' function serves as the main user interface. It's designed to generate multiple message variations in a single call, providing flexibility for researchers to choose the best response.

The function accepts several parameters: the base prompt, an optional system prompt for guided generation, additional information to append to the prompt, the number of message variations to generate, temperature for controlling randomness, and an verbose flag called "itinerary".

For each requested connection, it calls set_course() and collects the results. The function handles error checking and provides either just the generated messages or a complete list with the original prompts depending on the itinerary parameter.
:::

## Sample Usage! {.smaller}

Example System Prompt:

::: info-box
"You are a chatbot in an automated recovery support app for people with alcohol use disorder. You are writing a daily message from the app that will provide the person with information from a machine learning model about how they are doing in their recovery today. The message should be 3 to 4 sentences long and written for someone with an 8th grade reading level. Do not explicitly greet the person, and do not refer to yourself in the message. Do not provide advice."
:::

## Sample Usage

Example User Prompt:

::: highlight-box
"Write a message to tell this person that they are at low and increasing risk for drinking today. This person prefers messages that legitimize their distress by acknowledging their feelings as reasonable, validating that their experience is difficult, and assuring them it's okay to express these struggles."
:::

## Sample Usage! {auto-animate="true"}

``` r
pilot_message(
  base_prompt = prompt4a,
  system_prompt = sys_prompt,
  connections = 3,
  itinerary = TRUE
)
```

## Output Pt. 1 {.smaller auto-animate="true"}

::: highlight-box
`$system` \[1\] "You are a chatbot in an automated recovery support app for people with alcohol use disorder. You are writing a daily message from the app that will provide the person with information from a machine learning model about how they are doing in their recovery today. The message should be 3 to 4 sentences long and written for someone with an 8th grade reading level. Do not explicitly greet the person, and do not refer to yourself in the message. Do not provide advice."

`$prompt` \[1\] "Write a message to tell this person that they are at low and increasing risk for drinking today. This person prefers messages that legitimize their distress by acknowledging their feelings as reasonable, validating that their experience is difficult, and assuring them it's okay to express these struggles."
:::

## Output Pt. 2 {.smaller .scrollable auto-animate="true"}

::: info-box
`$response`

\[1\] "Some days can feel heavier than others, and it’s completely understandable to feel that way. The challenges you’re facing right now are real, and it’s okay to feel frustrated or overwhelmed. It’s hard work to navigate these moments, but acknowledging your struggles is a sign of strength. You’re allowed to feel what you’re feeling, and it’s okay to let those emotions out."

\[2\] "It looks like today might feel harder than usual, and the urge to drink could be growing. Feeling this way doesn’t mean you’re failing—it’s a natural part of the process, especially when things feel overwhelming. Recovery can be tough, and it’s okay to feel frustrated or worn down. Your feelings are valid, and it’s important to let yourself acknowledge them."

\[3\] "It looks like today might be harder for you, and that’s completely understandable. Feeling the pull toward drinking can be tough, especially when emotions or stress are running high. What you’re going through is valid, and it’s okay to acknowledge how challenging this feels. You don’t have to push those feelings away—letting them out can be part of moving through this moment."
:::

## Conclusion {.section-slide}

## Future Work and Next Steps

Technical enhancements:

-   Cache Support
-   Integration with A-CHESS systems

Methodological enhancements:

-   Tidy-text/NLP analysis of messages
-   Continued prompt engineering

::: notes
Looking ahead, there are several exciting directions for both technical and methodological enhancements. On the technical side, I am exploring adding cache support, so that if you run an identical prompt within an certain time-frame, you won't continually generate messages. Additionally, integrating the system directly with A-CHESS backend infrastructure is a necessary step in the future. From a methodological perspective, I'd like to develop ways to quantitatively measure differences within our prompting. Additionally, we are looking into running a Qualtrics panel to see if participants do systematically prefer a certain message style.
:::

## Thank you & Questions

::: info-box
-   Aviation-themed R interface for Azure API
-   Supports the lab's mission to optimize Smart A-CHESS
-   Enables large-scale, personalized message generation
-   Part of a broader innovation in digital therapeutics
:::

::: notes
To conclude, I've developed an aviation-themed R interface that connects to the Azure OpenAI API for automating the generation of support messages for the Smart A-CHESS digital therapeutic. This work supports the Curtin Lab's mission to optimize how digital therapeutics provide feedback to users, with the ultimate goal of improving engagement and clinical outcomes for people with alcohol use disorder. By enabling large-scale, personalized message generation, this system plays a key role in advancing the research and development of next-generation digital therapeutics. Thank you for your attention, and I'm happy to answer any questions about either the technical implementation or the broader research context.
:::
